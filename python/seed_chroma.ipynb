{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "docs = JSONLoader(jq_schema='.[]', file_path='./dataset.json', text_content=False).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed the Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x11ff95750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True))\n",
    "collection = client.get_or_create_collection(\"demo\")\n",
    "embeddings_function = OpenAIEmbeddings()\n",
    "\n",
    "Chroma().from_documents(\n",
    "    docs, embeddings_function, client=client, collection_name=\"demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='demo' id=UUID('076b2f76-68c6-4bd0-933e-41dfce297358') metadata=None tenant=None database=None\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True))\n",
    "db = Chroma(client=client, collection_name=\"demo\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "print(client.get_collection(\"demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The blog is about food, specifically focusing on topics like making pancakes.\\n\\nSources:\\n- https://demo.html/b\\n- https://demo.html/a'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"What is this blog about?\"\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo', temperature=0, verbose=True)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = db.as_retriever()\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Try to keep the answer concise. If the answer has code, ensure you provide the code in a markdown code block format. Add the source URLs provided in the context at the end of the answer. \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "display(rag_chain.invoke(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ansys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
